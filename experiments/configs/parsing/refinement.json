{
  "model": "Refinement",
  "refinement": {
    "num_layers": 1,
    "final_interpolation": 1.0,
    "aux_interpolation": 1.0,
    "use_separate_first_biaf": true,
    "use_prev_layer_output": true
  },
  "input": {
    "word_dim": 100,
    "pos_dim": 100,
    "char_dim": 100,
    "use_pos": true,
    "use_char": false
  },
  "biaffine":{
    "arc_mlp_dim": 500,
    "rel_mlp_dim": 100,
    "p_in": 0.33,
    "p_out": 0.33,
    "loss_interpolation": 0.2,
    "activation": "elu",
    "minimize_logp": false
  },
  "input_encoder": {
    "name": "FastLSTM",
    "num_layers": 3,
    "num_attention_heads": 4,
    "hidden_size": 400,
    "intermediate_size": 800,
    "hidden_act": "gelu",
    "dropout_type": "seq",
    "p_rnn": [0.33, 0.33],
    "embedding_dropout_prob": 0.33,
    "hidden_dropout_prob": 0.2,
    "inter_dropout_prob": 0.1,
    "attention_probs_dropout_prob": 0.1,
    "use_input_layer": true,
    "use_sin_position_embedding": false,
    "freeze_position_embedding": false,
    "initializer": "default"
  },
  "graph_encoder": {
    "name": "none",
    "num_layers": 1,
    "num_attention_heads": 4,
    "share_params": false,
    "only_value_weight": false,
    "hidden_size": 200,
    "intermediate_size": 800,
    "hidden_act": "gelu",
    "dropout_type": "seq",
    "embedding_dropout_prob": 0.33,
    "hidden_dropout_prob": 0.2,
    "inter_dropout_prob": 0.1,
    "attention_probs_dropout_prob": 0.1,
    "use_input_layer": true,
    "use_null_att_pos": false,
    "use_sin_position_embedding": false,
    "freeze_position_embedding": false,
    "initializer": "default",
    "encode_arc_type": "hard-1",
    "encode_rel_type": "none",
    "rel_dim": 100
  }
}
